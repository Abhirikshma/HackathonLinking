{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f8f35891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import uproot as uproot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d00cc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = uproot.open('/eos/cms/store/group/dpg_hgcal/comm_hgcal/hackathon/samples/close_by_double_pion/production/new_new_ntuples/ntuples_3933206_0.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a0b6b0a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ntuplizer;1',\n",
       " 'ntuplizer/simtrackstersSC;17',\n",
       " 'ntuplizer/simtrackstersSC;16',\n",
       " 'ntuplizer/simtrackstersCP;17',\n",
       " 'ntuplizer/simtrackstersCP;16',\n",
       " 'ntuplizer/tracksters;6',\n",
       " 'ntuplizer/tracksters;5',\n",
       " 'ntuplizer/clusters;4',\n",
       " 'ntuplizer/clusters;3',\n",
       " 'ntuplizer/graph;1',\n",
       " 'ntuplizer/candidates;1',\n",
       " 'ntuplizer/trackstersMerged;1',\n",
       " 'ntuplizer/associations;1',\n",
       " 'ntuplizer/tracks;1']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ebe253ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 | typename                 | interpretation                \n",
      "---------------------+--------------------------+-------------------------------\n",
      "NTrackstersMerged    | unknown                  | <UnknownInterpretation 'non...\n",
      "barycenter_x         | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "barycenter_y         | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "barycenter_z         | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "EV1                  | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "EV2                  | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "EV3                  | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "eVector0_x           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "eVector0_y           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "eVector0_z           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "sigmaPCA1            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "sigmaPCA2            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "sigmaPCA3            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "id_probabilities     | std::vector<std::vect... | AsObjects(AsVector(True, As...\n"
     ]
    }
   ],
   "source": [
    "trackstersclue3d = file[\"ntuplizer/tracksters\"]\n",
    "simtrackstersCP = file[\"ntuplizer/simtrackstersCP\"]\n",
    "graph = file[\"ntuplizer/graph\"]\n",
    "associations = file[\"ntuplizer/associations\"]\n",
    "candidates = file[\"ntuplizer/candidates\"]\n",
    "tsmerged = file[\"ntuplizer/trackstersMerged;1\"]\n",
    "tsmerged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5ceaca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ntuples from tree in file\n",
    "tracksterN = trackstersclue3d[\"NTracksters\"].array(library='np')\n",
    "x = trackstersclue3d[\"barycenter_x\"].array(library='np')\n",
    "y = trackstersclue3d[\"barycenter_y\"].array(library='np')\n",
    "z = trackstersclue3d[\"barycenter_z\"].array(library='np')\n",
    "rawE = trackstersclue3d[\"raw_energy\"].array(library='np')\n",
    "rawE_EM = trackstersclue3d[\"raw_em_energy\"].array(library='np')\n",
    "linkedInners = graph[\"linked_inners\"].array(library='np')\n",
    "linkedOuters = graph[\"linked_outers\"].array(library='np')\n",
    "simTracksters_CP_N = simtrackstersCP[\"stsCP_NTracksters\"].array(library='np')\n",
    "tsAssocMap = associations[\"tsCLUE3D_recoToSim_CP\"].array(library='np')\n",
    "tsAssocQual = associations[\"tsCLUE3D_recoToSim_CP_score\"].array(library='np')\n",
    "\n",
    "event = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cf04b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "input_folder = \"/eos/cms/store/group/dpg_hgcal/comm_hgcal/hackathon/samples/close_by_double_pion/production/new_new_ntuples/\"\n",
    "files = glob.glob(f\"{input_folder}/*ntuples_*.root\")\n",
    "\n",
    "calos = [ ]\n",
    "tracksters = [ ]\n",
    "associations = [ ]\n",
    "graph = [ ]\n",
    "\n",
    "i = 0\n",
    "N = 10000000\n",
    "for file in files[:3]:\n",
    "    if i >= N: break\n",
    "    i+=1\n",
    "    try:\n",
    "        print('.', end=\"\")\n",
    "        f = uproot.open(file)\n",
    "        t =  f[\"ntuplizer/tracksters\"]\n",
    "        calo = f[\"ntuplizer/simtrackstersCP\"]\n",
    "        ass = f[\"ntuplizer/associations\"]\n",
    "        gra = f[\"ntuplizer/graph\"]\n",
    "        calos.append(calo.arrays([\"stsCP_trackster_barycenter_eta\",\"stsCP_trackster_barycenter_phi\",\n",
    "                                  \"stsCP_barycenter_x\",\"stsCP_barycenter_y\",\"stsCP_barycenter_z\",\"stsCP_raw_energy\"]))\n",
    "        tracksters.append(t.arrays([\"NTracksters\",\"raw_energy\",\"raw_em_energy\", \"trackster_barycenter_eta\",\"trackster_barycenter_phi\",\n",
    "                                    \"barycenter_x\",\"barycenter_y\",\"barycenter_z\",\"id_probabilities\",\n",
    "                                    \"EV1\", \"EV2\", \"EV3\", \"eVector0_x\", \"eVector0_y\",\"eVector0_z\", \"sigmaPCA1\", \"sigmaPCA2\", \"sigmaPCA3\"]))\n",
    "        associations.append(ass.arrays([ \"tsCLUE3D_recoToSim_CP\", \"tsCLUE3D_recoToSim_CP_score\"]))\n",
    "        graph.append(gra.arrays([\"linked_inners\"]))\n",
    "    except:\n",
    "        print(\"error \", file)\n",
    "        \n",
    "df_calo = ak.concatenate(calos)\n",
    "df_track = ak.concatenate(tracksters)\n",
    "df_ass = ak.concatenate(associations)\n",
    "df_gra = ak.concatenate(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3d5c098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ ]\n",
    "Edges = [ ]\n",
    "Edges_labels = [ ]\n",
    "\n",
    "for ev in range(100):\n",
    "#Get the tracksters in the window of each calo particles\n",
    "# tracks_in_window = track_idx_inwindow[calo_idx_inwindow == calo_idx]\n",
    "# Get only those trackers by ID\n",
    "    trk_data = df_track[ev]\n",
    "    gra_data = df_gra[ev]\n",
    "    ass_data = df_ass[ev]\n",
    "    # Save the input variables\n",
    "    x_ev = ak.zip({\"raw_en\": trk_data.raw_energy, \n",
    "                'raw_em_energy': trk_data.raw_em_energy,\n",
    "                     \"barycenter_x\": trk_data.barycenter_x,\n",
    "                     \"barycenter_y\": trk_data.barycenter_y,\n",
    "                     \"barycenter_z\": trk_data.barycenter_z,\n",
    "                     \"EV1\": trk_data.EV1,\n",
    "                     \"EV2\": trk_data.EV2,\n",
    "                     \"EV3\": trk_data.EV3\n",
    "                    }  )\n",
    "    X.append(x_ev)\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    edges_labels = []\n",
    "    for i in range(trk_data.NTracksters):\n",
    "        nodes.append(i)\n",
    "        qualities = ass_data.tsCLUE3D_recoToSim_CP_score[i]\n",
    "        best_sts_i = ass_data.tsCLUE3D_recoToSim_CP[i][ak.argmin(qualities)]\n",
    "        best_sts_i = best_sts_i if qualities[best_sts_i]<0.1 else -1\n",
    "        for j in gra_data.linked_inners[i]:\n",
    "            edges.append([j,i])\n",
    "            qualities = ass_data.tsCLUE3D_recoToSim_CP_score[j]\n",
    "            best_sts_j = ass_data.tsCLUE3D_recoToSim_CP[j][ak.argmin(qualities)]\n",
    "            best_sts_j = best_sts_j if qualities[best_sts_j]<0.1 else -1\n",
    "            if best_sts_i == best_sts_j:\n",
    "                edges_labels.append(1)\n",
    "            else:\n",
    "                edges_labels.append(0)\n",
    "            \n",
    "    ed_np = np.array(edges).T\n",
    "    Edges.append(ed_np)\n",
    "    Edges_labels.append(edges_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "af13df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "42150992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matches [sts]:  [1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1]\n",
      "associated to_same: [[1, 4, 5, 13, 15, 16], [0, 2, 3, 6, 7, 8, 9, 10, 11, 12, 14, 17], []]\n",
      "number of TICLGraph edges:  81\n",
      "[[2, 0], [0, 3], [2, 3], [1, 4], [1, 5], [4, 5], [0, 6], [8, 6], [9, 6], [2, 6], [7, 6], [10, 6], [3, 6], [0, 7], [8, 7], [9, 7], [2, 7], [10, 7], [3, 7], [0, 8], [2, 8], [10, 8], [3, 8], [0, 9], [8, 9], [2, 9], [10, 9], [3, 9], [0, 10], [2, 10], [3, 10], [0, 11], [8, 11], [9, 11], [2, 11], [7, 11], [10, 11], [5, 12], [11, 12], [0, 12], [8, 12], [9, 12], [2, 12], [7, 12], [10, 12], [3, 12], [6, 12], [5, 13], [1, 13], [4, 13], [11, 14], [0, 14], [8, 14], [12, 14], [9, 14], [2, 14], [7, 14], [10, 14], [3, 14], [6, 14], [5, 15], [1, 15], [4, 15], [13, 15], [12, 15], [5, 16], [15, 16], [1, 16], [4, 16], [13, 16], [12, 16], [0, 17], [8, 17], [14, 17], [12, 17], [9, 17], [2, 17], [7, 17], [10, 17], [3, 17], [6, 17]]\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# Truth from TS->STS associations\n",
    "ts_best_matches = [] # STS (from CP) best matched to a TS, can also be -1 for no match  \n",
    "assoc_threshold = 0.1 # match only if < threshold\n",
    "to_same = [] # TSs matched to the same STS grouped\n",
    "\n",
    "        \n",
    "for i in range(tracksterN[event]):\n",
    "    qualities = tsAssocQual[event][i]\n",
    "    best_score = np.amin(qualities) #lowest score\n",
    "    assocmapnp = tsAssocMap[event][i]\n",
    "    best_score_sts = assocmapnp[np.argmin(qualities)] #sts with the lowest score\n",
    "    if best_score < assoc_threshold:\n",
    "        ts_best_matches.append(best_score_sts)\n",
    "    else:\n",
    "        ts_best_matches.append(-1)\n",
    "        \n",
    "        \n",
    "for sts in range(simTracksters_CP_N[event]+1):\n",
    "    to_same.append([])\n",
    "print(\"Best matches [sts]: \", ts_best_matches)\n",
    "for ts, sts in enumerate(ts_best_matches):\n",
    "    to_same[sts].append(ts)\n",
    "\n",
    "# tracksters not truth-matched are put in the last element of to_same \n",
    "print(\"associated to_same:\", to_same)\n",
    "\n",
    "\n",
    "# TICLGraph #\n",
    "\n",
    "edges = []\n",
    "nodes = []\n",
    "# pos in R-z, nx can't take 3D \n",
    "pos = {i:(z[event][i], np.sqrt(np.square(x[event][i]) + np.square(y[event][i]))) for i in range(tracksterN[event])}\n",
    "for i in range(tracksterN[event]):\n",
    "    nodes.append(i)\n",
    "    for j in linkedInners[event][i]:\n",
    "        edges.append([j,i])\n",
    "print(\"number of TICLGraph edges: \", len(edges))\n",
    "print(edges)\n",
    "        \n",
    "\n",
    "# TICLGraph truth #\n",
    "\n",
    "# label TICLGraph edge 1 if the two edges are from the same CP else 0\n",
    "\n",
    "edgesTICLtruth = []\n",
    "for i in range(tracksterN[event]):\n",
    "    for j in linkedInners[event][i]:\n",
    "        if ts_best_matches[i] == ts_best_matches[j]:\n",
    "            edgesTICLtruth.append(1)\n",
    "        else:\n",
    "            edgesTICLtruth.append(0)\n",
    "print(len(edgesTICLtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7f4b3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_np = np.array(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3127d290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0  2  1  1  4  0  8  9  2  7 10  3  0  8  9  2 10  3  0  2 10  3  0\n",
      "   8  2 10  3  0  2  3  0  8  9  2  7 10  5 11  0  8  9  2  7 10  3  6  5\n",
      "   1  4 11  0  8 12  9  2  7 10  3  6  5  1  4 13 12  5 15  1  4 13 12  0\n",
      "   8 14 12  9  2  7 10  3  6]\n",
      " [ 0  3  3  4  5  5  6  6  6  6  6  6  6  7  7  7  7  7  7  8  8  8  8  9\n",
      "   9  9  9  9 10 10 10 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 13\n",
      "  13 13 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 16 16 16 16 16 16 17\n",
      "  17 17 17 17 17 17 17 17 17]]\n"
     ]
    }
   ],
   "source": [
    "print(edges_np.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "29c7f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n",
    "\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "        pr\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)\n",
    "\n",
    "class GNNEdgeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNEdgeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a layer which takes a pair of nodes connected by an edge, \n",
    "        # passes their embeddings through an ffn to produce the edge_embeddings\n",
    "        self.edgespostprocess = create_ffn(hidden_units, dropout_rate, name=\"edgepostprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        \n",
    "        # Postprocess node embedding. <- being done in the edgepostprocess\n",
    "        # x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        # node_embeddings = tf.gather(x, input_node_indices)\n",
    "        \n",
    "        # need a way to return the node_embeddings of the nodes\n",
    "        # connected by the edges to be fed into an ffn that combines\n",
    "        # them and feed these 'edge_embeddings' into a classification \n",
    "        # layer that predicts a label/score for every edge\n",
    "        row = self.edges[0,:]\n",
    "        col = self.edges[1,:]\n",
    "        row_embed = tf.gather(x, row)\n",
    "        col_embed = tf.gather(x, col)\n",
    "        edge_embeddings = self.edgespostprocess(tf.concat([row_embed, col_embed],0))\n",
    "        # Compute logits\n",
    "        return self.compute_logits(edge_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "08948c95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_pad = 90\n",
    "Node_data = []\n",
    "Edge_data = [] # list of arrays !\n",
    "for ev in range(10):\n",
    "    X_ev = []\n",
    "    Edge_data.append(Edges[ev])\n",
    "    for fields in X[ev].fields:\n",
    "        X_ev.append(np.pad(ak.to_numpy(X[ev][fields]), (0, 90-len(X[ev][fields]))))\n",
    "    \n",
    "    Node_data.append(X_ev)\n",
    "Node_data = np.array(Node_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f7bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0cd54ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461\n",
      "Edges shape: (2, 1461)\n",
      "Nodes shape: (90, 8)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Dimensions of inputs should match: shape[0] = [90,32] vs. shape[1] = [63,32] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_507/3208905675.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GNN output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mgnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_507/2811736478.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_node_indices)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# Apply the first graph conv layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Skip connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_507/2811736478.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0maggregated_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbour_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Update the node embedding with the neighbour messages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_repesentations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregated_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGNNEdgeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_507/2811736478.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, node_repesentations, aggregated_messages)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombination_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"concat\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Concatenate the node_repesentations and aggregated_messages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_repesentations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregated_messages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombination_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Add node_repesentations and aggregated_messages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1766\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1767\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101cuda/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [90,32] vs. shape[1] = [63,32] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "# Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
    "edges = Edge_data[ev]\n",
    "print(edges.shape[1])\n",
    "# Create an edge weights array of ones.\n",
    "edge_weights = tf.ones(shape=edges.shape[1])\n",
    "# Create a node features array of shape [num_nodes, num_features].\n",
    "node_features = tf.cast(\n",
    "    Node_data[ev].T, dtype=tf.dtypes.float32\n",
    ")\n",
    "# Create graph info tuple with node_features, edges, and edge_weights.\n",
    "graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "print(\"Edges shape:\", edges.shape)\n",
    "print(\"Nodes shape:\", node_features.shape)\n",
    "    \n",
    "gnn_model = GNNEdgeClassifier(\n",
    "    graph_info=graph_info,\n",
    "    num_classes=2,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "print(\"GNN output shape:\", gnn_model(range(90)))\n",
    "\n",
    "gnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622592ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
