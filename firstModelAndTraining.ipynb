{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f35891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 10:36:50.377969: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import uproot as uproot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00cc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = uproot.open('/eos/cms/store/group/dpg_hgcal/comm_hgcal/hackathon/samples/close_by_double_pion/production/new_new_ntuples/ntuples_3933206_0.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b6b0a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ntuplizer;1',\n",
       " 'ntuplizer/simtrackstersSC;17',\n",
       " 'ntuplizer/simtrackstersSC;16',\n",
       " 'ntuplizer/simtrackstersCP;17',\n",
       " 'ntuplizer/simtrackstersCP;16',\n",
       " 'ntuplizer/tracksters;6',\n",
       " 'ntuplizer/tracksters;5',\n",
       " 'ntuplizer/clusters;4',\n",
       " 'ntuplizer/clusters;3',\n",
       " 'ntuplizer/graph;1',\n",
       " 'ntuplizer/candidates;1',\n",
       " 'ntuplizer/trackstersMerged;1',\n",
       " 'ntuplizer/associations;1',\n",
       " 'ntuplizer/tracks;1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe253ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 | typename                 | interpretation                \n",
      "---------------------+--------------------------+-------------------------------\n",
      "NTrackstersMerged    | unknown                  | <UnknownInterpretation 'non...\n",
      "barycenter_x         | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "barycenter_y         | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "barycenter_z         | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "EV1                  | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "EV2                  | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "EV3                  | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "eVector0_x           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "eVector0_y           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "eVector0_z           | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "sigmaPCA1            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "sigmaPCA2            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "sigmaPCA3            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\n",
      "id_probabilities     | std::vector<std::vect... | AsObjects(AsVector(True, As...\n"
     ]
    }
   ],
   "source": [
    "trackstersclue3d = file[\"ntuplizer/tracksters\"]\n",
    "simtrackstersCP = file[\"ntuplizer/simtrackstersCP\"]\n",
    "graph = file[\"ntuplizer/graph\"]\n",
    "associations = file[\"ntuplizer/associations\"]\n",
    "candidates = file[\"ntuplizer/candidates\"]\n",
    "tsmerged = file[\"ntuplizer/trackstersMerged;1\"]\n",
    "tsmerged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ceaca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ntuples from tree in file\n",
    "tracksterN = trackstersclue3d[\"NTracksters\"].array(library='np')\n",
    "x = trackstersclue3d[\"barycenter_x\"].array(library='np')\n",
    "y = trackstersclue3d[\"barycenter_y\"].array(library='np')\n",
    "z = trackstersclue3d[\"barycenter_z\"].array(library='np')\n",
    "rawE = trackstersclue3d[\"raw_energy\"].array(library='np')\n",
    "rawE_EM = trackstersclue3d[\"raw_em_energy\"].array(library='np')\n",
    "linkedInners = graph[\"linked_inners\"].array(library='np')\n",
    "linkedOuters = graph[\"linked_outers\"].array(library='np')\n",
    "simTracksters_CP_N = simtrackstersCP[\"stsCP_NTracksters\"].array(library='np')\n",
    "tsAssocMap = associations[\"tsCLUE3D_recoToSim_CP\"].array(library='np')\n",
    "tsAssocQual = associations[\"tsCLUE3D_recoToSim_CP_score\"].array(library='np')\n",
    "\n",
    "event = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42150992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matches [sts]:  [1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1]\n",
      "associated to_same: [[1, 4, 5, 13, 15, 16], [0, 2, 3, 6, 7, 8, 9, 10, 11, 12, 14, 17], []]\n",
      "number of TICLGraph edges:  81\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# Truth from TS->STS associations\n",
    "ts_best_matches = [] # STS (from CP) best matched to a TS, can also be -1 for no match  \n",
    "assoc_threshold = 0.1 # match only if < threshold\n",
    "to_same = [] # TSs matched to the same STS grouped\n",
    "\n",
    "        \n",
    "for i in range(tracksterN[event]):\n",
    "    qualities = tsAssocQual[event][i]\n",
    "    best_score = np.amin(qualities) #lowest score\n",
    "    assocmapnp = tsAssocMap[event][i]\n",
    "    best_score_sts = assocmapnp[np.argmin(qualities)] #sts with the lowest score\n",
    "    if best_score < assoc_threshold:\n",
    "        ts_best_matches.append(best_score_sts)\n",
    "    else:\n",
    "        ts_best_matches.append(-1)\n",
    "        \n",
    "        \n",
    "for sts in range(simTracksters_CP_N[event]+1):\n",
    "    to_same.append([])\n",
    "print(\"Best matches [sts]: \", ts_best_matches)\n",
    "for ts, sts in enumerate(ts_best_matches):\n",
    "    to_same[sts].append(ts)\n",
    "\n",
    "# tracksters not truth-matched are put in the last element of to_same \n",
    "print(\"associated to_same:\", to_same)\n",
    "\n",
    "\n",
    "# TICLGraph #\n",
    "\n",
    "edges = []\n",
    "nodes = []\n",
    "# pos in R-z, nx can't take 3D \n",
    "pos = {i:(z[event][i], np.sqrt(np.square(x[event][i]) + np.square(y[event][i]))) for i in range(tracksterN[event])}\n",
    "for i in range(tracksterN[event]):\n",
    "    nodes.append(i)\n",
    "    for j in linkedInners[event][i]:\n",
    "        edges.append([j,i])\n",
    "print(\"number of TICLGraph edges: \", len(edges))\n",
    "        \n",
    "\n",
    "# TICLGraph truth #\n",
    "\n",
    "# label TICLGraph edge 1 if the two edges are from the same CP else 0\n",
    "\n",
    "edgesTICLtruth = []\n",
    "for i in range(tracksterN[event]):\n",
    "    for j in linkedInners[event][i]:\n",
    "        if ts_best_matches[i] == ts_best_matches[j]:\n",
    "            edgesTICLtruth.append(1)\n",
    "        else:\n",
    "            edgesTICLtruth.append(0)\n",
    "print(len(edgesTICLtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c7f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_input = keras.Input(shape=(6,), name=\"node_input\")\n",
    "# given a list of edges calculate the edge features from\n",
    "# the inputs pairs joined by the edges using a trainable\n",
    "# MLP\n",
    "\n",
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n",
    "\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gated\":\n",
    "            self.update_fn = layers.GRU(\n",
    "                units=hidden_units,\n",
    "                activation=\"tanh\",\n",
    "                recurrent_activation=\"sigmoid\",\n",
    "                dropout=dropout_rate,\n",
    "                return_state=True,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)\n",
    "\n",
    "class GNNEdgeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes=2,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNEdgeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # need a way to return the node_embeddings of the nodes\n",
    "        # connected by the edges to be fed into an ffn that combines\n",
    "        # them and feed these 'edge_embeddings' into a classification \n",
    "        # layer that predicts a label/score for every edge\n",
    "        \n",
    "        # Compute logits\n",
    "        return self.compute_logits(node_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd54ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
